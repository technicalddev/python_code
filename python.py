# -*- coding: utf-8 -*-
"""Step1-DataCleanUp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZfmNMgsd_Fm-jv6OxPHJYg0GGTszbCyO

# Authentication and Importing Libraries
"""

!pip install --upgrade 'sqlalchemy<2.0'

# Authenticating credentials

from google.colab import auth,files
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)
# gc = gspread.authorize(GoogleCredentials.get_application_default())

# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)
from google.colab import drive
drive.mount('/content/drive')

# Importing all the necessary modules
import pandas as pd
# import pandas_profiling
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from pandas.plotting import scatter_matrix
from datetime import datetime,date
import gspread_dataframe as gd
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)

!pip install pymysql

#initialize mysql engine
from sqlalchemy import create_engine
import pymysql
engine = create_engine('mysql+pymysql://' + 'admin' + ':' + '1234mONEYTOr' + '@' + '3.108.47.186' + ':3306/' + 'Mobikwik7_20_06_2024')
engine.connect()

"""# Data CleanUp steps
 1. View the raw data where States and City will have lower case and Upper Case with same name value
 2.View the NA values in Numerical columns and replace it with median of that variable

"""



data_sheet=gc.open_by_url("https://docs.google.com/spreadsheets/d/12dHJeJNQOSmZt-Fer5GIzFxmd6gvu72oszRWPlVmwoU/edit#gid=1676606582").worksheet("Raw_data")
# Converting the data into a dataframe
raw_data1=gd.get_as_dataframe(data_sheet,evaluate_formulas=True)
raw_data1=raw_data1.iloc[:,:]
# raw_data1['PaylaterIDs']=float(raw_data1['PaylaterIDs'])
raw_data1.shape

"""->Raw data has 57 varibles and 74980 loan Ids"""

raw_data1.to_sql("raw_data",con=engine,if_exists='replace',index=False)

sql_query='select * from raw_data'

raw_data=pd.read_sql_query(sql_query,engine)
raw_data.shape

raw_data.head()

# Observing whether the last entries of the data were included.
raw_data.tail()

raw_data.columns

raw_data.shape

raw_data["Id"].nunique()

raw_data_1 = raw_data[raw_data ["POS"] <500]

raw_data_1.shape

raw_data_1.to_csv("Raw_data_Low_Value.csv")

"""Relevent data with POS more than 500 has 57220 loans

### Data Description
order_id is the primary key of the data
"""

raw_data.isnull().sum()

"""Aadhar name has majority null values and is not required for analysis, hence dropping variable "Aadhar Name"
"""

# raw_data.drop(["name_on_aadhar", 'gender_aadhar', 'dob_on_aadhar', 'agency_referral', 'equifax_score',
#                'crif_score', 'tu_score', 'disbursed_amount','delinquency_string'], axis = 1, inplace = True)

""""Address", 'Pin code', 'Phone number', 'User Email', 'Mode of communication',
               'Aadhar Name', 'Aadhar address', 'No. of Last 30 Days Calls', 'PAN (y/n)',
               'No. of Last 60 Days Calls', 'No. of Last 90 Days Calls',
               'No. of Last 120 Days Calls', 'No. of Last 150 Days Calls',
               'No. of Last 30 Days Connected Calls',
               'No. of Last 60 Days Connected Calls',
               'No. of Last 90 Days Connected Calls',
               'No. of Last 120 Days Connected Calls',
               'No. of Last 150 Days Connected Calls', 'Latest Calling Date' variables were dropped based on null values and domain knowledge
"""

raw_data.shape

duplicates = raw_data[raw_data.duplicated()]
duplicates

"""There are no duplicate entires"""

raw_data.isnull().sum()

raw_data["DPD"].mean()

raw_data["Income_Segment"] = raw_data["Income_Segment"].fillna("NA")

raw_data.dtypes

"""State CleanUp"""

raw_data['State']=raw_data['State'].str.lower()

raw_data['State']

raw_data['State']=raw_data['State'].astype(str)

# raw_data['approved_1k_date'].median()

raw_data['State'].value_counts()

"""Replacing the name of states with null values as "NA"
"""

# # raw_data["State"]= raw_data["State"].replace("pune", "maharashtra")
# raw_data['current_state']=raw_data["current_state"].replace("None", "NA")


# sorted(raw_data["current_state"].unique())

"""State variable is clean

City Clean up
"""

# raw_data['current_district']

raw_data.columns

raw_data['City']=raw_data['City'].str.lower()

raw_data['City']

# raw_data["current_district"] = raw_data["current_district"].fillna("NA")
sorted(raw_data["City"].unique())

"""Replacing the name of states with Correct spellings and keeping the numericals as "NA"
"""

pd.set_option('display.max_rows', None)

raw_data['City'].value_counts()

"""Replacing the name of states with Correct spellings and keeping the numericals as "NA"
"""

#replacing with correct spellings
# raw_data["City"]= raw_data["City"].replace("ahmed nagar", "ahmadnagar")
# raw_data["City"]= raw_data["City"].replace("bangalore ..", "bangalore")
# raw_data["City"]= raw_data["City"].replace("bangalore ", "bangalore")



# sorted(raw_data["City"].unique())

"""Occupation"""

raw_data['Occupation_Nature_of_Employment']=raw_data['Occupation_Nature_of_Employment'].str.lower()
raw_data['Occupation_Nature_of_Employment']=raw_data['Occupation_Nature_of_Employment'].fillna("NA")
sorted(raw_data["Occupation_Nature_of_Employment"].unique())

# raw_data["Occupation"]= raw_data["Occupation"].replace("salaried-access", 'salaried')
# raw_data["Occupation"]= raw_data["Occupation"].replace('student-access', "student")

raw_data['Occupation_Nature_of_Employment'].value_counts()

"""Gender"""

raw_data['Gender']=raw_data['Gender'].fillna("NA")

sorted(raw_data["Gender"].unique())

raw_data['Customer(Repeat_New)'].value_counts()

raw_data['Gender'].value_counts()

"""Acquisition Channel


"""

raw_data.dtypes

# raw_data["Acquisition Channel"]=raw_data["Acquisition Channel"].fillna("NA")
# sorted(raw_data["delinquency_string"].unique())

# raw_data['Acquisition Channel']=raw_data['Acquisition Channel'].str.lower()

# sorted(raw_data["Acquisition Channel"].unique())

"""AADHAR (y/n)   """

# raw_data["AADHAR (y/n)"]=raw_data["AADHAR (y/n)"].fillna("NA")
# sorted(raw_data["AADHAR (y/n)"].unique())

"""Last Feedback/Disposition  



"""

# raw_data['Last Feedback/Disposition']=raw_data['Last Feedback/Disposition'].str.lower()

# raw_data["Last Feedback/Disposition"]=raw_data["Last Feedback/Disposition"].fillna("NA")
# raw_data["Last Feedback/Disposition"].unique()

raw_data.isnull().sum()

raw_data.columns

"""Trying to understand in values in other variables for null values"""

# LRA=raw_data[['uuid - User unique id', 'order_id', 'writeoff_date',
#        'dpd_as_of_woff_dte', 'dpd_as_of_20221114', 'order_date', 'loan_amount',
#        'interest_rate', 'emi_amount', 'emi_tenure', 'total_interest_paid',
#        'total_principal_paid', 'total_repayment_amount',
#        'principal_outstanding', 'total_outstanding', 'last_repayment_date',
#        'last_repayment_amount', 'bureau_report_date', 'mss_30_flag',
#        'mss_60_flag', 'tu_credit_score', 'active_tradelines',
#        'active_tradelines_amount', 'total_accounts', 'delinquent_accts_count',
#        'delinquent_accts_amount', 'Age', 'Gender', 'State', 'City',
#        'Occupation', 'Acquisition Channel', 'AADHAR (y/n)',
#        'Email - no of emails', 'Phone - no of phone nos available',
#        'Last Feedback/Disposition', 'Date of last contact',
#        'Latest Connected Date']][raw_data["last_repayment_date"].isnull()]

# LRA.sample(30)

"""The loans with null values in Total Repayments are new loans (October 2022)"""

# Null_Category = ['bureau_report_date', 'Latest Connected Date']
# for value in Null_Category:
#     raw_data[value].fillna("NA",inplace=True)

# for value in ['mss_30_flag', 'mss_60_flag', 'last_repayment_amount']:
#     raw_data[value].fillna(0,inplace=True)

# for value in ['total_accounts', 'delinquent_accts_count']:
#     raw_data[value].fillna(1,inplace=True)

# raw_data["total_accounts"]= raw_data["total_accounts"].replace(0,1)
# raw_data["delinquent_accts_count"]= raw_data["delinquent_accts_count"].replace(0,1)

# for value in ["tu_credit_score", 'active_tradelines',
#        'active_tradelines_amount', 'delinquent_accts_amount', 'Age', 'Email - no of emails', 'Phone - no of phone nos available']:
#   raw_data[value].fillna(raw_data[value].median(), inplace=True)

raw_data["Occupation_Nature_of_Employment"]= raw_data["Occupation_Nature_of_Employment"].replace("not available", "NA")

raw_data['Occupation_Nature_of_Employment'].value_counts()

raw_data["Mobiscore_band"]= raw_data["Mobiscore_band"].replace("Not Classified", "NA")

raw_data['Mobiscore_band'].value_counts()

raw_data.isnull().sum()

"""There are no null values in the raw_data now"""

raw_data.head()

raw_data.dtypes

raw_data.to_sql("working_data",con=engine,if_exists='replace',index=False)

raw_data.to_csv("Working_data.csv", index = False)

"""# End Of data CleanUp

"""