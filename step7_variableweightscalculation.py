# -*- coding: utf-8 -*-
"""Step7-VariableWeightsCalculation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lAr7oRcPqbDPHAdo2JxSZCbgfwcQ1SWg

# Authentication and Importing Libraries
"""

!pip install pandas-profiling
!pip install --upgrade 'sqlalchemy<2.0'

# Authenticating credentials

from google.colab import auth,files
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)
# gc = gspread.authorize(GoogleCredentials.get_application_default())

# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Importing all the necessary modules
import pandas as pd
import pandas_profiling
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from pandas.plotting import scatter_matrix
from datetime import datetime,date

from sklearn import metrics
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression, RidgeCV, LassoCV, Ridge, Lasso
import pandas as pd
pd.options.display.float_format = '{:.4f}'.format
import seaborn as sns
import statsmodels.api as sm
import statsmodels

from sklearn.feature_selection import SelectKBest, chi2, f_classif,f_regression, RFE
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, ExtraTreesClassifier

# VIF Library
from statsmodels.stats.outliers_influence import variance_inflation_factor

# For clustering
from scipy.spatial import distance
from scipy.spatial.distance import cdist
from sklearn.metrics import silhouette_samples, silhouette_score
import sklearn.cluster as sc
from scipy.spatial import distance as d
from sklearn.manifold import TSNE
import plotly.graph_objs as go

# PCA
from sklearn.decomposition import PCA
import pylab as pl

# Supervised Learning
from sklearn.tree import DecisionTreeRegressor

# Decision Tree
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error
from sklearn.tree import plot_tree
import gspread_dataframe as gd


# Visualizations
import matplotlib.cm as cm
# %matplotlib inline

!pip install pymysql

#initialize mysql engine
from sqlalchemy import create_engine
import pymysql
engine = create_engine('mysql+pymysql://' + 'admin' + ':' + '1234mONEYTOr' + '@' + '3.108.47.186' + ':3306/' + 'navi_08_09_2023')
engine.connect()

#!pip install dython
#from dython import nominal

"""# Variable Weights Calculation Process"""

#Getting the working_data from Databases
sql_query='select * from normalised_data'
normalised_df=pd.read_sql_query(sql_query,engine)
normalised_df.shape

normalised_df.columns

normalised_df.isna().sum()

normalised_df=normalised_df.fillna(0)

normalised_df.dtypes

normalised_df

from statsmodels.stats.outliers_influence import variance_inflation_factor
# Checking VIF Scores
pd.options.display.float_format = '{:.3f}'.format

def calc_vif(X):

    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif.sort_values(by = 'VIF', inplace = False, ascending = False))

numerical_features= ['Validated income','Sanction amount','Amount disbursed','Count of previous loans', 'Arbitration',
       'Section 25', 'no of emis paid','tenure_months', 'annual_rate_of_interest', 'current_dpd',
       'first_npa_pos', 'last_npa_pos', 'bureau_score',
       'principal_outstanding_amount', 'interest_outstanding_amount',
       'bounce_fee_pending', 'late_fee_pending', 'penal_interest_pending',
       'penalty_charges_pending', 'Actual DPD (Calculated)', 'Total_Paid',
       'Days_since_disbursal', 'disp_code_NRTP', 'disp_code_address_not_found',
       'disp_code_connectivity_mode_problem', 'disp_code_paid',
       'disp_code_phone_not connected', 'disp_code_positive_response',
       'disp_code_settlement', 'TotalPercentPaid',
       'TimeWeightedTotalPercentPaid', 'Age', 'total_outstanding',
       'state_name', 'gender', 'employment_type',
       'Max_count_dispositionCode_map', 'Last_interaction_sub_type',
       'Last_customer_feedback', 'Last_disp_code_category', 'AgeBracket',
       'DPDSEGMENT','EMIsPaidSegmentation', 'CibilSegmentation','PreviousLoanTakenSegmentation', 'IncomeSegmentation','Count_of_Total_Paid']
numerical_features

# calc_vif(normalised_df[numerical_features]) and heatmap, based on it drop variables one by one and then rerun the same

# normalised_df.corr().to_csv("correlation_mobikwik2_23_11_2022.csv")

normalised_df = normalised_df[numerical_features]

normalised_df.shape

normalised_df.columns

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(30,10))
cor = normalised_df.corr()
Corr_heatmap=sns.heatmap(cor,cmap='icefire',annot=True)
plt.show()

correlation = numerical_features

calc_vif(normalised_df)

normalised_df = normalised_df[numerical_features]

relevant_columns=['Validated income',
                  # 'Sanction amount', 12 High corr with POS
                  # 'Amount disbursed', 4 POS
                  'Count of previous loans',
                  # 'Arbitration',
                  # 'Section 25',
                  'no of emis paid',
                  # 'tenure_months', 13 High corr with POS
                  'annual_rate_of_interest',
                  'current_dpd',
                  # 'first_npa_pos', 5 POS
                  # 'last_npa_pos', 2 POS
                  'bureau_score',
                  'principal_outstanding_amount',
                  # 'interest_outstanding_amount',9 POS high corr
                  'bounce_fee_pending',
                  # 'late_fee_pending', 10 high corr bounce_fee_pending and penal_interest_pending
                  'penal_interest_pending',
                  # 'penalty_charges_pending', 11 high corr POS
                  'Actual DPD (Calculated)',
                  # 'Total_Paid', 15 High corr with Days_since_disbursal
                  # 'Days_since_disbursal',
                  'disp_code_NRTP',
                  'disp_code_address_not_found',
                  'disp_code_connectivity_mode_problem',
                  'disp_code_paid',
                  'disp_code_phone_not connected',
                  'disp_code_positive_response',
                  'disp_code_settlement',
                  # 'TotalPercentPaid', 7 Time weightedpercent paid
                  'TimeWeightedTotalPercentPaid',
                  'Age',
                  # 'total_outstanding', 1 POS
                  'state_name',
                  'gender',
                  # 'employment_type', 6 POS
                  'Max_count_dispositionCode_map',
                  'Last_interaction_sub_type',
                  'Last_customer_feedback',
                  # 'Last_disp_code_category', 8 Last customer feedback high corr
                  # 'AgeBracket', 3 HIGH VIF
                  'DPDSEGMENT',
                  # 'EMIsPaidSegmentation',
                  'CibilSegmentation',
                  # 'PreviousLoanTakenSegmentation', 14 High VIF
                  'IncomeSegmentation',
                  # 'Count_of_Total_Paid'
                  ]

plt.figure(figsize=(30,10))
cor = normalised_df[relevant_columns].corr()
Corr_heatmap=sns.heatmap(cor,cmap='icefire',annot=True)
plt.show()

calc_vif(normalised_df[relevant_columns])

normalised_df[relevant_columns].corr().to_csv("correlation_mobikwik2_23_11_2022.csv")

"""Loan Amount, Total Repayment Amount and Address Type dropped based on VIF"""

# normalised_df =normalised_df.drop("Total Repayment Amount", axis = 1)
# normalised_df =normalised_df.drop('Borrower Identifier', axis = 1)
# normalised_df =normalised_df.drop('Loan Amount', axis = 1)
# normalised_df =normalised_df.drop('Address Type', axis = 1)
# normalised_df =normalised_df.drop('Total Outstanding', axis = 1)
# normalised_df =normalised_df.drop('PercentPaid', axis = 1)
# normalised_df =normalised_df.drop('Age_years', axis = 1)
# normalised_df =normalised_df.drop('EMI Amount', axis = 1)
# normalised_df =normalised_df.drop('DPD', axis = 1)
# normalised_df =normalised_df.drop('Total Interest Paid', axis = 1)

final_df = normalised_df[relevant_columns]

final_df.shape

# checking pairplot of every variable with percent paid to understand outliers in data

sns.pairplot(final_df,y_vars="TimeWeightedTotalPercentPaid", x_vars=final_df)

data_new=final_df.copy()

data_new.columns

len(data_new)

normalised_df.columns

final_df.columns

df_for_weights=final_df.copy()

# TimeWeightedPercentPaid is first column as we need the importance of other variables for this
# Select the columns which are necessary for analysis

# remove

column_list =['Validated income', 'Count of previous loans', 'no of emis paid',
       'annual_rate_of_interest', 'current_dpd', 'bureau_score',
       'principal_outstanding_amount', 'bounce_fee_pending',
       'penal_interest_pending', 'Actual DPD (Calculated)', 'disp_code_NRTP',
       'disp_code_address_not_found', 'disp_code_connectivity_mode_problem',
       'disp_code_paid', 'disp_code_phone_not connected',
       'disp_code_positive_response', 'disp_code_settlement',
       'TimeWeightedTotalPercentPaid', 'Age', 'state_name', 'gender',
       'Max_count_dispositionCode_map', 'Last_interaction_sub_type',
       'Last_customer_feedback', 'DPDSEGMENT', 'CibilSegmentation',
       'IncomeSegmentation']

noOfColumns = len(column_list)

# Xcolumns are all without PercentPaid in the same order
Xcolumns = column_list[1: ]

print(noOfColumns)

df_for_weights=df_for_weights[column_list]

Xcolumns

df_for_weights.head()

array =df_for_weights.values

print('\n--array shape--\n')

print(array.shape)

X = array[:, 1:noOfColumns]
Y = array[:, 0]

plt.figure(figsize=(30,10))
cor = df_for_weights.corr()
Corr_heatmap=sns.heatmap(cor,cmap='icefire',annot=True)
plt.show()

from sklearn.feature_selection import SelectKBest, chi2, f_classif,f_regression, RFE
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, ExtraTreesClassifier

# FEATURE EXTRACTION - ANOVA - Classification Feature Selection: (Numerical Input, Categorical Output)

print('\n--K Best features--\n')
test = SelectKBest(score_func=f_classif, k=4)
fit = test.fit(X,Y)
np.set_printoptions(precision=6)
print('\n--features importance--\n')
print(fit.scores_)
kBestFeatures = pd.DataFrame({'features': Xcolumns})
kBestFeatures['importance'] = fit.scores_
kBestFeatures = kBestFeatures.sort_values('importance', ascending=False)
b = []
a = sum(fit.scores_)
for i in fit.scores_:
  b.append(i/a)
b = pd.DataFrame(b)
kBestFeatures['relative importance'] = b
print(kBestFeatures)
features = fit.transform(X)
kBestFeatures.to_csv("KBESTFeatures.csv")
# summarize selected features
# print(features[0:5,:])

# REGRESSION FEATURE SELECTION: (NUMERICAL INPUT, NUMERICAL OUTPUT)
print('\n--Regression Feature Selection: (Numerical Input, Numerical Output)-\n')
test = SelectKBest(score_func=f_regression, k=4)
fit = test.fit(X, Y)
print(fit.scores_)
regressionFeatures = pd.DataFrame({'features': Xcolumns})
regressionFeatures['importance'] = fit.scores_
regressionFeatures = regressionFeatures.sort_values('importance', ascending=False)
b = []
a = float(sum(fit.scores_))
for i in fit.scores_:
  b.append((i/a))
b = pd.DataFrame(b)
regressionFeatures['relative importance'] = b
print(regressionFeatures)
regressionFeatures.to_csv("RegressionFeature.csv")

# ExtraTreesRegressor
print('\n--ExtraTreesRegressor Best features--\n')
model = ExtraTreesRegressor(n_estimators=100)
model.fit(X, Y)
print('\n--features importance--\n')
print(model.feature_importances_)
featuresImportance = pd.DataFrame({'features': Xcolumns})
featuresImportance['importance'] = model.feature_importances_
featuresImportance = featuresImportance.sort_values('importance', ascending=False)
b = []
a = sum(fit.scores_)
for i in fit.scores_:
  b.append(i/a)
b = pd.DataFrame(b)
featuresImportance['relative importance'] = b
print(featuresImportance)
featuresImportance.to_csv("ExtraTreeRegressor.csv")

# df_for_weights["Borrower Identifier"]= normalised_df["Borrower Identifier"]

df_for_weights.reset_index(inplace=True,drop=True)

df_for_weights

s=df_for_weights.corr()["TimeWeightedTotalPercentPaid"]
s

dic_for_polarity={}
for x in s.index:
  if s[x]>0:
    dic_for_polarity[x]=1
  else:
    if s[x]<0:
      dic_for_polarity[x]=-1
    else:
      dic_for_polarity[x]=0
dic_for_polarity

# regressionFeatures['features']
weight_df=regressionFeatures[['features','relative importance']]
weight_df=weight_df.set_index("features",drop=True)
weight_df

# weight_df.to_csv("weights_features.csv")

weight_dic={}
for x in dic_for_polarity.keys():
  if x!='TimeWeightedTotalPercentPaid':
    weight_dic[x]=weight_df["relative importance"][x]
weight_dic

# TimeWeightedTotalPercentPaid weight=25%
for x in weight_dic.keys():
    weight_dic[x]*=0.75
weight_dic["TimeWeightedTotalPercentPaid"]=0.25
weight_dic

weight_dic

final_weights={}
for x in weight_dic.keys():
  final_weights[x]=weight_dic[x]*dic_for_polarity[x]
final_weights

final_weights

