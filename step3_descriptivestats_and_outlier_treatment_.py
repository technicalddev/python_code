# -*- coding: utf-8 -*-
"""Step3-DescriptiveStats and outlier treatment .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xZy1Kco1AksdoRKOtfXcuKcWIqTRyIFE

# Authentication and Importing Libraries
"""

!pip install pandas-profiling
!pip install --upgrade 'sqlalchemy<2.0'

# Authenticating credentials

from google.colab import auth,files
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)
# gc = gspread.authorize(GoogleCredentials.get_application_default())

# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Importing all the necessary modules
import pandas as pd
import pandas_profiling
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from pandas.plotting import scatter_matrix
from datetime import datetime,date

from sklearn import metrics
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression, RidgeCV, LassoCV, Ridge, Lasso
import pandas as pd
pd.options.display.float_format = '{:.4f}'.format
import seaborn as sns
import statsmodels.api as sm
import statsmodels

from sklearn.feature_selection import SelectKBest, chi2, f_classif,f_regression, RFE
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, ExtraTreesClassifier

# VIF Library
from statsmodels.stats.outliers_influence import variance_inflation_factor

# For clustering
from scipy.spatial import distance
from scipy.spatial.distance import cdist
from sklearn.metrics import silhouette_samples, silhouette_score
import sklearn.cluster as sc
from scipy.spatial import distance as d
from sklearn.manifold import TSNE
import plotly.graph_objs as go

# PCA
from sklearn.decomposition import PCA
import pylab as pl

# Supervised Learning
from sklearn.tree import DecisionTreeRegressor

# Decision Tree
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error
from sklearn.tree import plot_tree
import gspread_dataframe as gd


# Visualizations
import matplotlib.cm as cm
# %matplotlib inline

#!pip install dython
#from dython import nominal
!pip install pymysql

#initialize mysql engine
from sqlalchemy import create_engine
import pymysql
engine = create_engine('mysql+pymysql://' + 'admin' + ':' + '1234mONEYTOr' + '@' + '3.108.47.186' + ':3306/' + 'navi_08_09_2023')
engine.connect()

"""#Exploratory Analysis"""

# #Getting the working_data from Databases
sql_query='select * from working_data_08_09_2023'
working_data=pd.read_sql_query(sql_query,engine)
working_data.shape

working_data

#List Of categorical Columns and Numerical Columns(Except target variable)

cat=[]
num=[]
for x in working_data.columns[1:]:
    if working_data[x].dtype=="object":
        cat.append(x)
    else:
        num.append(x)
print(f"Total Catergorical variables are {len(cat)}.",  f"Total Numerical variables are {len(num)}." )

num

cat

# data_new = raw_data-Cat_df
data_new=working_data[working_data.columns[~working_data.columns.isin(cat)]]
data_new.shape

# %matplotlib inline
data_new.plot(kind='box' , figsize = (25, 50), subplots=True, layout=(13,3), sharex=False)
plt.show()

# k = len(data_new.columns)
# n = 2
# m = (k - 1) // n + 1
# fig, axes = plt.subplots(m, n, figsize=(n * 8, m * 4))
# for i, (name, col) in enumerate(data_new.iteritems()):
#     r, c = i // n, i % n
#     ax = axes[r, c]
#     col.hist(ax=ax)
#     ax2 = col.plot.kde(ax=ax, secondary_y=True, title=name)
#     ax2.set_ylim(0)
# fig.tight_layout()

"""##Distribution Analysis

"""

sns.distplot(working_data['principal_outstanding_amount'], bins=25, kde=True )

sns.distplot(working_data['total_outstanding'], bins=25, kde=True )

sns.distplot(working_data['Total_Paid'], bins=25, kde=True )

sns.distplot(working_data['TotalPercentPaid'], bins=25, kde=True )

sns.distplot(working_data['TimeWeightedTotalPercentPaid'], bins=25, kde=True )

sns.distplot(working_data['Age'], bins=25, kde=True )

sns.distplot(working_data['Actual DPD (Calculated)'], bins=25, kde=True )

sns.distplot(working_data['bureau_score'].where(working_data['bureau_score']!="NA"), bins=25, kde=True )

sns.distplot(working_data['Validated income'], bins=25, kde=True )

sns.distplot(working_data['Count of previous loans'], bins=25, kde=True )

sns.distplot(working_data['no of emis paid'], bins=25, kde=True )

from statsmodels.stats.outliers_influence import variance_inflation_factor
# Checking VIF Scores
pd.options.display.float_format = '{:.3f}'.format

def calc_vif(X):

    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif.sort_values(by = 'VIF', inplace = False, ascending = False))

numerical_features=working_data.columns[1:]
numerical_features

# calc_vif and heatmap, based on it drop variables one by one and then rerun the same

working_data.corr().to_csv("correlation_Navi.csv")

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(30,10))
cor = working_data.corr()
Corr_heatmap=sns.heatmap(cor,cmap='icefire',annot=True)
plt.show()

# calc_vif(working_data[numerical_features])

working_data_1= working_data.copy()

working_data.shape

working_data_1.columns

relevant_columns=[ 'Validated income', 'Sanction amount',
       'Amount disbursed', 'Count of previous loans',  'tenure_months', 'annual_rate_of_interest',
       'state_name', 'current_dpd', 'last_npa_pos',
       'gender', 'employment_type', 'bureau_score',
       'principal_outstanding_amount', 'interest_outstanding_amount',
       'bounce_fee_pending', 'late_fee_pending', 'penal_interest_pending',
       'penalty_charges_pending', 'Actual DPD (Calculated)', 'Total_Paid',
       'Days_since_disbursal', 'disp_code_NRTP', 'disp_code_address_not_found',
       'disp_code_connectivity_mode_problem', 'disp_code_paid',
       'disp_code_phone_not connected', 'disp_code_positive_response',
       'disp_code_settlement', 'disp_code_special',
       'Max_count_dispositionCode_map', 'TotalPercentPaid', 'Last_interaction_sub_type',
       'Last_customer_feedback', 'Last_feedback_date',
       'Last_disp_code_category', 'Age', 'AgeBracket', 'DPDSEGMENT',
       'CibilSegmentation', 'total_outstanding', 'PreviousLoanTakenSegmentation', 'IncomeSegmentation','no of emis paid','Arbitration',
       'Section 25','PreviousLoanTakenSegmentation', 'Count_of_Total_Paid','Last_Payment_Date']

sns.pairplot(working_data_1,y_vars="TimeWeightedTotalPercentPaid", x_vars=relevant_columns)

working_data_1.shape

working_data_1["Amount disbursed"] = working_data_1["Amount disbursed"].where(working_data_1["Amount disbursed"]<750000)
working_data_1["tenure_months"] = working_data_1["tenure_months"].where(working_data_1["tenure_months"]<65)
working_data_1["TimeWeightedTotalPercentPaid"] = working_data_1["TimeWeightedTotalPercentPaid"].where(working_data_1["TimeWeightedTotalPercentPaid"]<0.004)
working_data_1["last_npa_pos"] = working_data_1["last_npa_pos"].where(working_data_1["last_npa_pos"]<700000)
working_data_1["principal_outstanding_amount"] = working_data_1["principal_outstanding_amount"].where(working_data_1["principal_outstanding_amount"]<700000)
working_data_1["penal_interest_pending"] = working_data_1["penal_interest_pending"].where(working_data_1["penal_interest_pending"]<5000)
working_data_1["penalty_charges_pending"] = working_data_1["penalty_charges_pending"].where(working_data_1["penalty_charges_pending"]<60000)
working_data_1["Total_Paid"] = working_data_1["Total_Paid"].where(working_data_1["Total_Paid"]<550000)
working_data_1["disp_code_paid"] = working_data_1["disp_code_paid"].where(working_data_1["disp_code_paid"]<4.5)
working_data_1["disp_code_settlement"] = working_data_1["disp_code_settlement"].where(working_data_1["disp_code_settlement"]<4.5)
working_data_1["total_outstanding"] = working_data_1["total_outstanding"].where(working_data_1["total_outstanding"]<800000)
working_data_1["Validated income"] = working_data_1["Validated income"].where(working_data_1["Validated income"]<500000)
working_data_1["Sanction amount"] = working_data_1["Sanction amount"].where(working_data_1["Sanction amount"]<800000)
working_data_1["Count of previous loans"] = working_data_1["Count of previous loans"].where(working_data_1["Count of previous loans"]<20)
working_data_1["Section 25"] = working_data_1["Section 25"].where(working_data_1["Section 25"]=='No')

sns.pairplot(working_data_1,y_vars="TimeWeightedTotalPercentPaid", x_vars=relevant_columns)

working_data_1.shape

working_data.dtypes

working_data_1.isnull().sum()

working_data_1.to_sql("Treated_data",con=engine, if_exists='replace',index=False)

working_data.to_csv("Treated_data.csv")

