# -*- coding: utf-8 -*-
"""Step2-DerivedVariables.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ETLjPJUc8ZdC2ANf3nOZ7MPwUniljqXs

# Authentication and Importing Libraries
"""

!pip install pandas-profiling
!pip install --upgrade 'sqlalchemy<2.0'

# Authenticating credentials

from google.colab import auth,files
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)
# gc = gspread.authorize(GoogleCredentials.get_application_default())

# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Importing all the necessary modules
import pandas as pd
import pandas_profiling
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from pandas.plotting import scatter_matrix
from datetime import datetime,date

from sklearn import metrics
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression, RidgeCV, LassoCV, Ridge, Lasso
import pandas as pd
pd.options.display.float_format = '{:.4f}'.format
import seaborn as sns
import statsmodels.api as sm
import statsmodels

from sklearn.feature_selection import SelectKBest, chi2, f_classif,f_regression, RFE
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, ExtraTreesClassifier

# VIF Library
from statsmodels.stats.outliers_influence import variance_inflation_factor

# For clustering
from scipy.spatial import distance
from scipy.spatial.distance import cdist
from sklearn.metrics import silhouette_samples, silhouette_score
import sklearn.cluster as sc
from scipy.spatial import distance as d
from sklearn.manifold import TSNE
import plotly.graph_objs as go

# PCA
from sklearn.decomposition import PCA
import pylab as pl

# Supervised Learning
from sklearn.tree import DecisionTreeRegressor

# Decision Tree
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error
from sklearn.tree import plot_tree
import gspread_dataframe as gd


# Visualizations
import matplotlib.cm as cm
# %matplotlib inline

#!pip install dython
#from dython import nominal
!pip install pymysql

#initialize mysql engine
from sqlalchemy import create_engine
import pymysql
engine = create_engine('mysql+pymysql://' + 'admin' + ':' + '1234mONEYTOr' + '@' + '3.108.47.186' + ':3306/' + 'navi_08_09_2023')
engine.connect()

"""# Preprocessing"""

# #Getting the working_data from Databases
sql_query='select * from working_data_08_09_2023'
working_data=pd.read_sql_query(sql_query,engine)
working_data.shape

working_data.describe()

# # Getting values from the sheet
# data_sheet=gc.open_by_url("https://docs.google.com/spreadsheets/d/1ZMMF9khIrIl3G2-of7IniXDOThyHuDB_otip9BhfbJY/edit#gid=1233629614").worksheet("working_data_22_11_2022 (1)")
# # Converting the data into a dataframe
# working_data=gd.get_as_dataframe(data_sheet,evaluate_formulas=True)
# working_data=working_data.iloc[:211209,:]
# # raw_data1['PaylaterIDs']=float(raw_data1['PaylaterIDs'])
# working_data.shape

working_data.shape

working_data.columns

"""### Data Description
Borrower Identifier is the primary key of the data



"""

# Check for NaN values
working_data.isnull().sum()

working_data.dtypes

working_data['disbursed_date'] = pd.to_datetime(working_data['disbursed_date'])
working_data['maturity_date'] = pd.to_datetime(working_data['maturity_date'])
working_data['write_off_date'] = pd.to_datetime(working_data['write_off_date'])
working_data['first_npa_date'] = pd.to_datetime(working_data['first_npa_date'])
working_data['last_npa_date'] = pd.to_datetime(working_data['last_npa_date'])
working_data['date_of_birth'] = pd.to_datetime(working_data['date_of_birth'])

#List Of categorical Columns and Numerical Columns(Except target variable)

cat=[]
num=[]
for x in working_data.columns[2:]:
    if working_data[x].dtype=="object":
        cat.append(x)
    else:
        num.append(x)
print(f"Total Catergorical variables are {len(cat)}.",  f"Total Numerical variables are {len(num)}." )

num

cat

"""#Derived Variables

Percent Paid
"""

repayment_query="select * from collection_data"
repayment_data=pd.read_sql_query(repayment_query,engine)
repayment_data.shape

repayment_data.head()

disposition_query="select * from disposition_code_working_data"
disposition_data=pd.read_sql_query(disposition_query,engine)
disposition_data.shape
disposition_data.head()

disposition_data_pivot = disposition_data[['account_number', 'disp_code_category']]
disposition_data_pivot.head()

disposition_data_pivot = disposition_data[['account_number', 'disp_code_category']]

# Pivot the DataFrame
pivot_table = pd.DataFrame(pd.pivot_table(disposition_data_pivot, index='account_number', columns='disp_code_category', aggfunc=len, fill_value=0))

# Display the pivot table
pivot_table.head()

Disposition_workingData=pivot_table.to_csv("Disposition_workingData.csv")

columns_to_sum = ['principal_paid', 'interest_paid','late_fee_paid','bf_paid','pi_paid','penalty_paid']
repayment_data['Total_Paid'] = repayment_data[columns_to_sum].sum(axis=1)

repayment_data.to_sql('collection_data_working',con=engine,if_exists='replace',index=False)

repayment_query="select account_number,sum(Total_Paid) AS 'Total_Paid' from collection_data_working group by account_number;"
repayment_data=pd.read_sql_query(repayment_query,engine)
repayment_data.shape

working_data = working_data.merge(repayment_data, on='account_number', how='left')

working_data['Total_Paid'].sum()

current_date = datetime.strptime("2023-09-08", '%Y-%m-%d')
working_data['Days_since_disbursal'] = ((current_date -  working_data['disbursed_date'])).astype(str).str.replace(' days', '')
working_data['Days_since_disbursal']=working_data['Days_since_disbursal'].astype(int)

# working_data['Day_since_last_disbursal']=date.today() - working_data['disbursed_date']

working_data.dtypes

Disp_workingData = gc.open_by_url("https://docs.google.com/spreadsheets/d/1WMzXKcQk6XSNjon7gF7czzUW0Zb0jGzGGqmp-VP3NtI/edit#gid=674715722").worksheet("Disposition_workingData")
Disposition_workingData = gd.get_as_dataframe(Disp_workingData,evaluate_formulas=True)

Disposition_workingData

working_data = working_data.merge(Disposition_workingData, on='account_number', how='left')

working_data_1 = working_data[working_data["disp_code_special"]==0]

# Change above code from ==0 to >0 to get the data of deceased customer loan IDs
Loans_of_deceased_customer=working_data_1.to_csv("Loans_of_deceased_customer.csv")

working_data_1.shape

working_data_2 = working_data_1[working_data_1["Total_Paid"]>0]

# working_data_2 contains data excluding deceased customer and loan Ids with Zero Repayments
working_data_2.shape

# Change above code from >0 to ==0 to get the data of Loans_with_zero_repayment_ExcludingDeceased
Loans_with_zero_repayment_ExcludingDeceased=working_data_2.to_csv("Loans_with_zero_repayment_ExcludingDeceased.csv")

working_data = working_data_2

working_data['TotalPercentPaid']=working_data['Total_Paid']/working_data['Amount disbursed']

working_data.dtypes

working_data["TimeWeightedTotalPercentPaid"]=working_data["TotalPercentPaid"]/working_data["Days_since_disbursal"]
# working_data["TimeWeightedTotalPercentPaid"]=working_data["TimeWeightedTotalPercentPaid"].astype(float)

working_data

working_data.shape

Disp_workingData_1 = gc.open_by_url("https://docs.google.com/spreadsheets/d/148DG28h2kkzOI4g8LDHGwlB6N9IJ4FRzUk1ueJ7ria4/edit#gid=1460499833").worksheet("Sheet1")
Disposition_workingData_1 = gd.get_as_dataframe(Disp_workingData_1,evaluate_formulas=True)

Disposition_workingData_1.columns

Disposition_workingData_2 = Disposition_workingData_1[Disposition_workingData_1["rnk"]==1]
Disposition_workingData_2.shape

Disposition_workingData_2.rename(columns={'interaction_sub_type': 'Last_interaction_sub_type', 'customer_feedback': 'Last_customer_feedback','feedback_date': 'Last_feedback_date','disp_code_category': 'Last_disp_code_category'}, inplace=True)

Disposition_workingData_2.drop(columns=['rnk'], inplace=True)
Disposition_workingData_2.columns

Disposition_workingData_2.columns

working_data = working_data.merge(Disposition_workingData_2, on='account_number', how='left')

working_data_1 = working_data[working_data["disp_code_NRTP"]>0]

NRTP_Customer_with_some_repayments=working_data_1.to_csv("NRTP_Customer_with_some_repayments.csv")

# working_data["Disbural Date"]=pd.to_datetime(working_data["Disbural Date"])
# today_date=datetime.today()
# date=pd.to_datetime("2022-11-07")
# print(date)
# working_data["DaysSinceLastisbursal"]=working_data["DaysSinceLastisbursal"].dt.da
# working_data["LoanAge"]=date-working_data["DaysSinceLastisbursal"]
# working_data["LoanAge"]=working_data["LoanAge"].dt.days

# working_data["LoanAge"].astype(int)
# working_data["LoanAge"]

"""Time Weighted Percent Paid"""

# working_data["TimeWeightedPercentPaid"]= working_data["PercentPaid"]/working_data["LoanAge"]

"""Age"""

working_data.dtypes

current_date = datetime.strptime("2023-09-08", '%Y-%m-%d')
working_data['Age'] = ((current_date -  working_data['date_of_birth'])).astype(str).str.replace(' days', '')
working_data['Age']=(working_data['Age'].astype(int))/365
working_data['Age']= (working_data['Age'].round()).astype(int)

working_data["Age"].max()

def ageBracketCalculator(weight):
  LowerBounds=[0,20,30,40,50,60]
  UpperBounds=[20,30,40,50,60,999]
  ageBracket=['0-20','20-30','30-40','40-50','50-60','60+']
  for i in range(len(ageBracket)):
    if weight=="Null":
      ageFactor="Null"
    elif weight< UpperBounds[i] and weight>= LowerBounds[i]:
      ageFactor = ageBracket[i]
  return ageFactor

working_data["AgeBracket"]=working_data["Age"].apply(lambda x:ageBracketCalculator(x))

"""Loanage and DPD segments"""

def dpdbucketCalculator(weight):
  LowerBounds=[0,180,200,220,240, 260, 280]
  UpperBounds=[180,200,220,240, 260, 280,1999]
  dpdbucket=['0-180','180-200','200-220','220-240','240-260','260-280', '280+']
  for i in range(len(dpdbucket)):
    if weight=="Null":
      dpdFactor="Null"
    elif weight< UpperBounds[i] and weight>= LowerBounds[i]:
      dpdFactor = dpdbucket[i]
  return dpdFactor

working_data["DPDSEGMENT"]=working_data["Actual DPD (Calculated)"].apply(lambda x:dpdbucketCalculator(x))

working_data["DPDSEGMENT"].unique()

working_data['no of emis paid'].describe()

def EMIsPaidCalculator(weight):
  LowerBounds=[0,3,4,6]
  UpperBounds=[3,4,6,100]
  dpdbucket=['0-3','3-4','4-6','6+']
  for i in range(len(dpdbucket)):
    if weight=="Null":
      EMIsPaidFactor="Null"
    elif weight< UpperBounds[i] and weight>= LowerBounds[i]:
      EMIsPaidFactor = dpdbucket[i]
  return EMIsPaidFactor

working_data["EMIsPaidSegmentation"]=working_data["no of emis paid"].apply(lambda x:EMIsPaidCalculator(x))

working_data[["EMIsPaidSegmentation","no of emis paid"]]

# working_data["LoanAge"].max()

# def loanagebucketCalculator(weight):
#   LowerBounds=[0,200,400,600,800,1000,1200,1400,1600, 1800]
#   UpperBounds=[200,400,600,800,1000,1200,1400,1600, 1800, 2000]
#   loanagebucket=['0-200','200-400','400-600','600-800','800-1000','1000-1200','1200-1400','1400-1600', '1600-1800','1800+']
#   for i in range(len(loanagebucket)):
#     if weight=="Null":
#       loanageFactor="Null"
#     elif weight< UpperBounds[i] and weight>= LowerBounds[i]:
#       loanageFactor = loanagebucket[i]
#   return loanageFactor

# working_data["LoanageSegment"]=working_data["LoanAge"].apply(lambda x:loanagebucketCalculator(x))

"""Cibil Segmentation"""

working_data["bureau_score"].dtypes

def cibilscoreCalculator(weight):
  LowerBounds=[0,500,570, 640, 710,780, 850]
  UpperBounds=[500,570, 640, 710,780, 850, 999]
  cibilBracket=["0 - 500", "500-570", "570-640", "640-710", "710-780", "780-850", "850+"]
  if isinstance(weight, str) and weight == "NA":
      return "NA"
  try:
      weight = int(weight)
      for i in range(len(UpperBounds)):
          if weight < UpperBounds[i] and weight >= LowerBounds[i]:
              return cibilBracket[i]
  except ValueError:
      return "Unknown"  # Handle non-integer values with this default
  return "Unknown"  # Handle any other unexpected cases with this default

working_data["CibilSegmentation"]=working_data["bureau_score"].apply(lambda x:cibilscoreCalculator(x))

working_data['CibilSegmentation'].unique()

working_data.shape

columns_to_sum = ['principal_outstanding_amount', 'interest_outstanding_amount','bounce_fee_pending','late_fee_pending','penal_interest_pending','penalty_charges_pending']
working_data['total_outstanding'] = working_data[columns_to_sum].sum(axis=1)

working_data.isna().sum()

working_data['Last_customer_feedback']=working_data['Last_customer_feedback'].fillna('NA')
working_data['Last_disp_code_category']=working_data['Last_disp_code_category'].fillna('NA')

working_data['Validated income'].describe()

def PreviousLoanTakenCalculator(weight):
  LowerBounds=[0,1,2,3,4,5]
  UpperBounds=[1,2,3,4,5,999]
  PreviousLoanBracket=['0-1','1-2','2-3','3-4','4-5','5+']
  for i in range(len(PreviousLoanBracket)):
    if weight=="Null":
      PreviousLoanTakenFactor="Null"
    elif weight< UpperBounds[i] and weight>= LowerBounds[i]:
      PreviousLoanTakenFactor = PreviousLoanBracket[i]
  return PreviousLoanTakenFactor

working_data["PreviousLoanTakenSegmentation"]=working_data["Count of previous loans"].apply(lambda x:PreviousLoanTakenCalculator(x))

def IncomeCalculator(weight):
  LowerBounds=[0,17000,25000,35000]
  UpperBounds=[17000,25000,35000,1000000]
  IncomeBracket=['0-17000','17000-25000','25000-35000','35000+']
  for i in range(len(IncomeBracket)):
    if weight=="Null":
      IncomeFactor="Null"
    elif weight< UpperBounds[i] and weight>= LowerBounds[i]:
      IncomeFactor = IncomeBracket[i]
  return IncomeFactor

working_data["IncomeSegmentation"]=working_data["Validated income"].apply(lambda x:IncomeCalculator(x))

working_data["PreviousLoanTakenSegmentation"].unique()

working_data.isna().sum()

repayment_query_2="select account_number,count(Total_Paid) AS 'Count_of_Total_Paid' from collection_data_working group by account_number;"
repayment_data_2=pd.read_sql_query(repayment_query_2,engine)
repayment_data_2.shape

repayment_data_2

working_data = working_data.merge(repayment_data_2, on='account_number', how='left')

repayment_query_3="select account_number,max(month) AS 'Last_Payment_Date' from collection_data_working group by account_number;"
repayment_data_3=pd.read_sql_query(repayment_query_3,engine)
repayment_data_3.shape

repayment_data_3

working_data = working_data.merge(repayment_data_3, on='account_number', how='left')

working_data[["Count_of_Total_Paid","no of emis paid","account_number"]]

working_data

working_data.to_csv("working_data_2023_09_12.csv", index = False)

working_data.to_sql("working_data_08_09_2023",con=engine,if_exists='replace',index=False)

"""End Of Derived Variables"""