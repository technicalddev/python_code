# -*- coding: utf-8 -*-
"""Step9-PriceCalculation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UcDyaLy8GGVjizz4OkkHL1PRVrvWR-VF

# Authentication and Importing Libraries
"""

!pip install pandas-profiling
!pip install --upgrade 'sqlalchemy<2.0'

# Authenticating credentials

from google.colab import auth,files
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)
# gc = gspread.authorize(GoogleCredentials.get_application_default())

# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Importing all the necessary modules
import pandas as pd
import pandas_profiling
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from pandas.plotting import scatter_matrix
from datetime import datetime,date

from sklearn import metrics
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression, RidgeCV, LassoCV, Ridge, Lasso
import pandas as pd
pd.options.display.float_format = '{:.4f}'.format
import seaborn as sns
import statsmodels.api as sm
import statsmodels

from sklearn.feature_selection import SelectKBest, chi2, f_classif,f_regression, RFE
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, ExtraTreesClassifier

# VIF Library
from statsmodels.stats.outliers_influence import variance_inflation_factor

# For clustering
from scipy.spatial import distance
from scipy.spatial.distance import cdist
from sklearn.metrics import silhouette_samples, silhouette_score
import sklearn.cluster as sc
from scipy.spatial import distance as d
from sklearn.manifold import TSNE
import plotly.graph_objs as go

# PCA
from sklearn.decomposition import PCA
import pylab as pl

# Supervised Learning
from sklearn.tree import DecisionTreeRegressor

# Decision Tree
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error
from sklearn.tree import plot_tree
import gspread_dataframe as gd
pd.set_option("display.max_columns", None)

# Visualizations
import matplotlib.cm as cm
# %matplotlib inline

#!pip install dython
#from dython import nominal

!pip install pymysql

#initialize mysql engine
from sqlalchemy import create_engine
import pymysql
engine = create_engine('mysql+pymysql://' + 'admin' + ':' + '1234mONEYTOr' + '@' + '3.108.47.186' + ':3306/' + 'navi_08_09_2023')
engine.connect()

"""# Price Calculation"""

#Getting the raw_data from Databases
sql_query='select * from Treated_data'
raw_data=pd.read_sql_query(sql_query,engine)
raw_data.shape

raw_data.dtypes

#Getting the potential_to_pay from Databases
sql_query='select * from potentialToPay'
potential_to_pay=pd.read_sql_query(sql_query,engine)
potential_to_pay.shape

potential_to_pay.columns

#raw_data,normalised_df,normalised_sigma,potential_to_pay

final_data=pd.DataFrame()
final_data["loan_account_id"]=raw_data["loan_account_id"]
final_data["account_number"]=raw_data["account_number"]
final_data["scaledPotentialToPay"]=potential_to_pay["ScaledPotentialToPay"]
final_data["Principal Outstanding"]=raw_data["principal_outstanding_amount"]
final_data["Total Outstanding"]=raw_data["total_outstanding"]
final_data["TimeWeightedTotalPercentPaid"]=raw_data["TimeWeightedTotalPercentPaid"]
final_data["BureauScore"]=raw_data["bureau_score"]
final_data["DaysSinceDisbursal"]=raw_data["Days_since_disbursal"]
final_data["State"]=raw_data["state_name"]
final_data['LoanAge']=raw_data['Days_since_disbursal']
final_data["pricePercent"]=potential_to_pay["pricePercent"]

final_data

#final_data.to_csv("final_data.csv")

#final_data.to_excel("final_data.xlsx")

#final_data.to_csv("final_smartcoin_pricing_updated_without_EMIspaid_and_unpaid_EMIs.csv",index=False)

workableStates_sheet=gc.open_by_url("https://docs.google.com/spreadsheets/d/1S-eIPysk15MRoMeIRL_6SLitLk46QyjNgJkNRUtGp8c/edit#gid=1030244678").worksheet("WorkableStates")
workableStates=gd.get_as_dataframe(workableStates_sheet,evaluate_formulas=True, header = 1)
workableStates=workableStates.iloc[:36,:2]
workableStates.shape

workableStates

final_data1=pd.merge(final_data,workableStates,on ='State', how ='left')
final_data1=final_data1.rename(columns={'Weight': 'WorkableStates'})
final_data=final_data1

final_data["Principal Outstanding"]=raw_data["principal_outstanding_amount"]
final_data["price"]=final_data["pricePercent"]*final_data["Principal Outstanding"]

#CutOff=10%/Median of LoanAge
CutOff=0.05/final_data["LoanAge"].median()

final_data["TimeWeightedTotalPercentPaid>=CutOff"]=final_data["TimeWeightedTotalPercentPaid"].apply(lambda x: 1 if x>=CutOff else 0)

final_data["Principal Outstanding>2000"]=final_data["Principal Outstanding"].apply(lambda x: 1 if x>=2000 else 0)
final_data

final_data["Principal Outstanding<75000"]=final_data["Principal Outstanding"].apply(lambda x: 1 if x<75000 else 0)
final_data

final_data.isnull().sum()

final_data=final_data.fillna(1)

final_data["DPDBucket1"]=final_data["DaysSinceDisbursal"].apply(lambda x: 1 if x<365  else 0)
final_data["DPDBucket2"]=final_data["DaysSinceDisbursal"].apply(lambda x: 1 if (x>365 and x<530) else 0)
final_data["DPDBucket3"]=final_data["DaysSinceDisbursal"].apply(lambda x: 1 if (x>530 and x<730) else 0)
final_data["DPDBucket4"]=final_data["DaysSinceDisbursal"].apply(lambda x: 1 if (x>730 and x<1090) else 0)
final_data["DPDBucket5"]=final_data["DaysSinceDisbursal"].apply(lambda x: 1 if x>1090 else 0)
final_data

def dpdbucketCalculator(weight):
  LowerBounds=[0,365,530,730]
  UpperBounds=[365,530,730,1090]
  dpdbucket=[1,2,3,4]
  for i in range(len(dpdbucket)):
    if weight=="Null":
      dpdFactor="Null"
    elif weight< UpperBounds[i] and weight>= LowerBounds[i]:
      dpdFactor = dpdbucket[i]
  return dpdFactor

final_data["DPDBucket"]=final_data["DaysSinceDisbursal"].apply(lambda x:dpdbucketCalculator(x))

final_data



final_data.to_csv("finalData.csv")

final_data["usingBuckets1-7"]=final_data["pricePercent"].apply(lambda x: 0 if x<=0.02988 else 1)
final_data["usingBuckets1-5"]=final_data["pricePercent"].apply(lambda x: 0 if x<=0.05857 else 1)
final_data["usingBuckets1-4"]=final_data["pricePercent"].apply(lambda x: 0 if x<=0.081997 else 1)
final_data

final_data["Base1"]=final_data["TimeWeightedTotalPercentPaid>=CutOff"]*final_data["Principal Outstanding>2000"]*final_data["Principal Outstanding<75000"]*final_data["usingBuckets1-7"]
final_data["Base2"]=final_data["TimeWeightedTotalPercentPaid>=CutOff"]*final_data["Principal Outstanding>2000"]*final_data["Principal Outstanding<75000"]*final_data["usingBuckets1-5"]
final_data["Base3"]=final_data["TimeWeightedTotalPercentPaid>=CutOff"]*final_data["Principal Outstanding>2000"]*final_data["Principal Outstanding<75000"]*final_data["usingBuckets1-4"]

final_data

final_data["Base1+workableStates"]=final_data["Base1"]*final_data["WorkableStates"]
final_data["Base2+workableStates"]=final_data["Base2"]*final_data["WorkableStates"]
final_data["Base3+workableStates"]=final_data["Base3"]*final_data["WorkableStates"]

final_data

final_data.to_csv("FinalPrice.csv")

final_data.to_sql("final_price",con=engine, if_exists='replace',index=False)

