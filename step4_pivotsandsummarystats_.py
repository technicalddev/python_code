# -*- coding: utf-8 -*-
"""Step4-PivotsAndSummaryStats .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oVKSimwQnbTdNGzNnsfgNwSuXbf08fUG

# Authentication and Importing Libraries
"""

!pip install pandas-profiling
!pip install --upgrade 'sqlalchemy<2.0'

# Authenticating credentials

from google.colab import auth,files
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)
# gc = gspread.authorize(GoogleCredentials.get_application_default())

# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Importing all the necessary modules
import pandas as pd
import pandas_profiling
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from pandas.plotting import scatter_matrix
from datetime import datetime,date

from sklearn import metrics
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression, RidgeCV, LassoCV, Ridge, Lasso
import pandas as pd
pd.options.display.float_format = '{:.4f}'.format
import seaborn as sns
import statsmodels.api as sm
import statsmodels

from sklearn.feature_selection import SelectKBest, chi2, f_classif,f_regression, RFE
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, ExtraTreesClassifier

# VIF Library
from statsmodels.stats.outliers_influence import variance_inflation_factor

# For clustering
from scipy.spatial import distance
from scipy.spatial.distance import cdist
from sklearn.metrics import silhouette_samples, silhouette_score
import sklearn.cluster as sc
from scipy.spatial import distance as d
from sklearn.manifold import TSNE
import plotly.graph_objs as go

# PCA
from sklearn.decomposition import PCA
import pylab as pl

# Supervised Learning
from sklearn.tree import DecisionTreeRegressor

# Decision Tree
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error
from sklearn.tree import plot_tree
import gspread_dataframe as gd
pd.set_option('display.max_columns', None)

# Visualizations
import matplotlib.cm as cm
# %matplotlib inline

#!pip install dython
#from dython import nominal

!pip install pymysql

#initialize mysql engine
from sqlalchemy import create_engine
import pymysql
engine = create_engine('mysql+pymysql://' + 'admin' + ':' + '1234mONEYTOr' + '@' + '3.108.47.186' + ':3306/' + 'navi_08_09_2023')
engine.connect()

"""#Pivot"""

#Getting the working_data from Databases
sql_query='select * from Treated_data'
working_data=pd.read_sql_query(sql_query,engine)
working_data.shape

working_data.dtypes

working_data.head()

working_data.columns

working_data.columns=['loan_account_id', 'date', 'account_number', 'disbursed_date',
       'maturity_date', 'Validated income', 'Sanction amount',
       'Amount disbursed', 'Count of previous loans', 'Arbitration',
       'Section 25', 'no of emis paid', 'tenure_months',
       'annual_rate_of_interest', 'state_name', 'pin_code', 'current_dpd',
       'write_off_date', 'first_npa_date', 'last_npa_date', 'first_npa_pos',
       'last_npa_pos', 'gender', 'employment_type', 'date_of_birth',
       'bureau_score', 'principal_outstanding_amount',
       'interest_outstanding_amount', 'bounce_fee_pending', 'late_fee_pending',
       'penal_interest_pending', 'penalty_charges_pending',
       'Actual DPD (Calculated)', 'Total_Paid', 'Days_since_disbursal',
       'disp_code_NRTP', 'disp_code_address_not_found',
       'disp_code_connectivity_mode_problem', 'disp_code_paid',
       'disp_code_phone_not connected', 'disp_code_positive_response',
       'disp_code_settlement', 'disp_code_special',
       'Max_count_dispositionCode_map', 'TotalPercentPaid',
       'TimeWeightedTotalPercentPaid', 'Last_interaction_sub_type',
       'Last_customer_feedback', 'Last_feedback_date',
       'Last_disp_code_category', 'Age', 'AgeBracket', 'DPDSEGMENT',
       'EMIsPaidSegmentation', 'CibilSegmentation', 'total_outstanding',
       'PreviousLoanTakenSegmentation', 'IncomeSegmentation',
       'Count_of_Total_Paid','Last_Payment_Date']

"""Finding mean, median, count for all the categorical columns.
Creating a list with the values of all pivot tables for categorical columns

"""

categorical = ['PreviousLoanTakenSegmentation', 'IncomeSegmentation','state_name', 'gender', 'employment_type', 'Max_count_dispositionCode_map', 'Last_interaction_sub_type','Last_customer_feedback','Last_disp_code_category','AgeBracket','DPDSEGMENT','CibilSegmentation','Arbitration',
       'Section 25','EMIsPaidSegmentation','principal_outstanding_amount','TimeWeightedTotalPercentPaid' ]

Cat_df = pd.DataFrame()
Cat_df = working_data[categorical]
Cat_df.head()

Cat_df.shape

Cat_Pivot=[]
for i in categorical:
  if i == "TimeWeightedTotalPercentPaid":
    break
  elif i == "principal_outstanding_amount":
    break
  else:
    table =pd.pivot_table(Cat_df, values=['TimeWeightedTotalPercentPaid','principal_outstanding_amount'], index=[i], aggfunc={'mean','median','count','sum'})
    print(table)
    Cat_Pivot.append(table)

Cat_Pivot

from google.colab import drive
drive.mount('/content/drive')

for x in Cat_Pivot:
  x.to_csv("/content/drive/MyDrive/Documents/Navi_Pivots/"+str(x.index.name)+".csv")

"""#Summary & Stats"""

summary=working_data.iloc[:,3:].describe()
summary

summary['bureau_score']

summary.shape

working_data.isna().sum()

#Summary.columns
num =['Validated income', 'Sanction amount',
       'Amount disbursed', 'Count of previous loans', 'tenure_months',
       'annual_rate_of_interest', 'current_dpd', 'first_npa_pos',
       'last_npa_pos', 'principal_outstanding_amount',
       'interest_outstanding_amount', 'bounce_fee_pending', 'late_fee_pending',
       'penal_interest_pending', 'penalty_charges_pending',
       'Actual DPD (Calculated)', 'Total_Paid', 'Days_since_disbursal',
       'disp_code_NRTP', 'disp_code_address_not_found',
       'disp_code_connectivity_mode_problem', 'disp_code_paid',
       'disp_code_phone_not connected', 'disp_code_positive_response',
       'disp_code_settlement', 'TotalPercentPaid',
       'TimeWeightedTotalPercentPaid', 'Age',  'total_outstanding','bureau_score','Count_of_Total_Paid','no of emis paid']

"""Concating "sum" of all variables to the summary stats"""

summary = pd.concat([summary, pd.DataFrame(working_data[num].sum(), columns = ["sum"]).T])
# summary = pd.concat([summary, pd.DataFrame(working_data[num].mode(), columns = ["mode"]).T])

summary

# summary.drop(["count","25%","75%"],axis=0,inplace=True)

Cat_Pivot

categorical_cols=[]
for x in Cat_Pivot:
  li=[]
  li.append(np.sum(x["TimeWeightedTotalPercentPaid"]["count"].values))
  li.append(np.mean(x["TimeWeightedTotalPercentPaid"]["median"].values))
  li.append(np.std(x["TimeWeightedTotalPercentPaid"]["median"].values))
  li.append(np.min(x["TimeWeightedTotalPercentPaid"]["median"].values))
  li.append((x["TimeWeightedTotalPercentPaid"]["median"]).quantile(0.25))
  li.append(np.median(x["TimeWeightedTotalPercentPaid"]["median"].values))
  li.append((x["TimeWeightedTotalPercentPaid"]["median"]).quantile(0.75))
  li.append(np.max(x["TimeWeightedTotalPercentPaid"]["median"].values))
  li.append(np.sum(x["TimeWeightedTotalPercentPaid"]["sum"].values))

  summary[x.index.name]=li
  categorical_cols.append(x.index.name)

categorical_cols

summary

summary.shape

summary.to_sql('summary_stats',con=engine,if_exists='replace',index=False)

summary.to_csv("Navi_SummaryStats.csv")

"""End of summarystats and Pivots"""

