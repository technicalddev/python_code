# -*- coding: utf-8 -*-
"""Step5-Normalization .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IYe1Q8FlJM2ZkNIZrUAn4PcTjss1tLuq

# Authentication and Importing Libraries
"""

!pip install pandas-profiling
!pip install --upgrade 'sqlalchemy<2.0'

# Authenticating credentials

from google.colab import auth,files
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)
# gc = gspread.authorize(GoogleCredentials.get_application_default())

# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Importing all the necessary modules
import pandas as pd
import pandas_profiling
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from pandas.plotting import scatter_matrix
from datetime import datetime,date

from sklearn import metrics
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression, RidgeCV, LassoCV, Ridge, Lasso
import pandas as pd
pd.options.display.float_format = '{:.4f}'.format
import seaborn as sns
import statsmodels.api as sm
import statsmodels

from sklearn.feature_selection import SelectKBest, chi2, f_classif,f_regression, RFE
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, ExtraTreesClassifier

# VIF Library
from statsmodels.stats.outliers_influence import variance_inflation_factor

# For clustering
from scipy.spatial import distance
from scipy.spatial.distance import cdist
from sklearn.metrics import silhouette_samples, silhouette_score
import sklearn.cluster as sc
from scipy.spatial import distance as d
from sklearn.manifold import TSNE
import plotly.graph_objs as go

# PCA
from sklearn.decomposition import PCA
import pylab as pl

# Supervised Learning
from sklearn.tree import DecisionTreeRegressor

# Decision Tree
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error
from sklearn.tree import plot_tree
import gspread_dataframe as gd
pd.set_option('display.max_columns', None)


# Visualizations
import matplotlib.cm as cm
# %matplotlib inline

#!pip install dython
#from dython import nominal

!pip install pymysql

#initialize mysql engine
from sqlalchemy import create_engine
import pymysql
engine = create_engine('mysql+pymysql://' + 'admin' + ':' + '1234mONEYTOr' + '@' + '3.108.47.186' + ':3306/' + 'navi_08_09_2023')
engine.connect()

"""#Normalised Data"""

#Take summary from gsheet
summary_sheet=gc.open_by_url("https://docs.google.com/spreadsheets/d/18BleV9Tj_6OGniDS-R8WsYEvEI_QXEKNZH8x3zMcjsw/edit#gid=909408802").worksheet("Navi_SummaryStats")
# Converting the data into a dataframe
summary=gd.get_as_dataframe(summary_sheet,evaluate_formulas=True)
summary=summary.iloc[:5,1:]
summary.shape

summary

summary.columns

summary['bureau_score']

columns_for_normalisation= list(summary.columns[:])

columns_for_normalisation

len(columns_for_normalisation)

#Getting the working_data from Databases
sql_query='select * from working_data_08_09_2023'
working_data=pd.read_sql_query(sql_query,engine)
working_data.shape



working_data.isna().sum()

# # Getting values from the sheet
# data_sheet=gc.open_by_url("https://docs.google.com/spreadsheets/d/1ZMMF9khIrIl3G2-of7IniXDOThyHuDB_otip9BhfbJY/edit#gid=1233629614").worksheet("working_data_22_11_2022 (1)")
# # Converting the data into a dataframe
# working_data=gd.get_as_dataframe(data_sheet,evaluate_formulas=True)
# working_data=working_data.iloc[:211209,:]
# # raw_data1['PaylaterIDs']=float(raw_data1['PaylaterIDs'])
# working_data.shape

working_data.sample()

working_data

columns_for_normalisation = list(working_data.columns)

working_data=working_data[columns_for_normalisation]

"""Normalization function (MinMaxScaler)"""

def func(x,mini,maxi):
  if x!=0:
    return (x-mini)/(maxi-mini)
  else:
    return 0

#Some values will come more than 1 and less than 0 as well
#We will impute the values which are more than 1 as 1
#Impute the values which are less than 0 as 0

working_data

working_data.columns

working_data.columns = ['loan_account_id', 'date', 'account_number', 'disbursed_date',
       'maturity_date', 'Validated income', 'Sanction amount',
       'Amount disbursed', 'Count of previous loans', 'Arbitration',
       'Section 25', 'no of emis paid', 'tenure_months',
       'annual_rate_of_interest', 'state_name', 'pin_code', 'current_dpd',
       'write_off_date', 'first_npa_date', 'last_npa_date', 'first_npa_pos',
       'last_npa_pos', 'gender', 'employment_type', 'date_of_birth',
       'bureau_score', 'principal_outstanding_amount',
       'interest_outstanding_amount', 'bounce_fee_pending', 'late_fee_pending',
       'penal_interest_pending', 'penalty_charges_pending',
       'Actual DPD (Calculated)', 'Total_Paid', 'Days_since_disbursal',
       'disp_code_NRTP', 'disp_code_address_not_found',
       'disp_code_connectivity_mode_problem', 'disp_code_paid',
       'disp_code_phone_not connected', 'disp_code_positive_response',
       'disp_code_settlement', 'disp_code_special',
       'Max_count_dispositionCode_map', 'TotalPercentPaid',
       'TimeWeightedTotalPercentPaid', 'Last_interaction_sub_type',
       'Last_customer_feedback', 'Last_feedback_date',
       'Last_disp_code_category', 'Age', 'AgeBracket', 'DPDSEGMENT',
       'EMIsPaidSegmentation', 'CibilSegmentation', 'total_outstanding',
       'PreviousLoanTakenSegmentation', 'IncomeSegmentation',
       'Count_of_Total_Paid', 'Last_Payment_Date']

categorical = ['date','account_number','disbursed_date','maturity_date','pin_code','write_off_date','first_npa_date','last_npa_date','Last_feedback_date','date_of_birth','state_name', 'gender', 'employment_type', 'Max_count_dispositionCode_map', 'Last_interaction_sub_type','Last_customer_feedback','Last_disp_code_category','AgeBracket','DPDSEGMENT','CibilSegmentation', 'PreviousLoanTakenSegmentation', 'IncomeSegmentation','Arbitration',
       'Section 25','EMIsPaidSegmentation','Last_Payment_Date']

categorical_1 = ['state_name', 'gender', 'employment_type', 'Max_count_dispositionCode_map', 'Last_interaction_sub_type','Last_customer_feedback','Last_disp_code_category','AgeBracket','DPDSEGMENT','CibilSegmentation', 'PreviousLoanTakenSegmentation', 'IncomeSegmentation','Arbitration',
       'Section 25','EMIsPaidSegmentation','principal_outstanding_amount','TimeWeightedTotalPercentPaid']

Cat_df = pd.DataFrame()
Cat_df = working_data[categorical_1]
Cat_df.head()

Cat_Pivot=[]
for i in categorical_1:
  if i == "TimeWeightedTotalPercentPaid":
    break
  elif i=='principal_outstanding_amount':
    break
  else:
    table =pd.pivot_table(Cat_df, values=['TimeWeightedTotalPercentPaid','principal_outstanding_amount'], index=[i],
                aggfunc={'mean','median','count','sum'})
    Cat_Pivot.append(table)

Cat_Pivot

categorical_cols=[]
for x in Cat_Pivot:
  li=[]
  # li.append(np.sum(x["TimeWeightedTotalPercentPaid"]["count"].values))
  li.append(np.mean(x["TimeWeightedTotalPercentPaid"]["median"].values))
  li.append(np.std(x["TimeWeightedTotalPercentPaid"]["median"].values))
  li.append(np.min(x["TimeWeightedTotalPercentPaid"]["median"].values))
  li.append(np.max(x["TimeWeightedTotalPercentPaid"]["median"].values))
  li.append(np.sum(x["TimeWeightedTotalPercentPaid"]["sum"].values))
  summary[x.index.name]=li
  categorical_cols.append(x.index.name)

summary.columns

categorical_cols

working_data_1=working_data

summary['state_name'].iloc[3]

categorical_cols

for y,z in zip(categorical_cols,Cat_Pivot):
    MAX=summary[y].iloc[3]
    MIN=summary[y].iloc[2]
    dic={}
    for x in z.index:
      dic[x]=float(z["TimeWeightedTotalPercentPaid"]["median"][x])
    dic[0]=0
    working_data[y]= working_data[y].apply(lambda x: func(dic[x],MIN,MAX))
    print(y)

working_data_1

# for y,z in zip(categorical,Cat_Pivot):
#   MAX=summary[y].iloc[7]
#   MIN=summary[y].iloc[3]
#   dic={}
#   for x in z.index:
#     dic[x]=float(z["TimeWeightedTotalPercentPaid"]["mean"][x])
#   dic[0]=0
#   working_data[y]=working_data[y].apply(lambda x: func(dic[x],MIN,MAX))
#   print(y)

summary

working_data.columns

working_data.isna().sum()

for x in working_data.columns:
  if x=="loan_account_id":
    pass
  else:
    if x not in categorical:
      MIN=float(summary[x].iloc[2])
      MAX=float(summary[x].iloc[3])
      working_data[x]=working_data[x].apply(lambda x: func(float(x),MIN,MAX))
      print(x)

working_data

Check = pd.DataFrame({"min":working_data.min(),"max": working_data.max()}).T

Check

"""**NOTE: ** It is important to impute the values which are more than 1 to 1 and less than 0 to zero.
Basically, we are imputing the outliers by bring them to the boundary of accepatable range of max and min.
Advantage of doing so is that we do not loose all variables of that datapoint, we just impute the variable which is having extreme values
"""

working_data["Amount disbursed"].mask(working_data["Amount disbursed"]>1,1,inplace=True)
working_data["tenure_months"].mask(working_data["tenure_months"]>1,1,inplace=True)
working_data["last_npa_pos"].mask(working_data["last_npa_pos"]>1,1,inplace=True)
working_data["principal_outstanding_amount"].mask(working_data["principal_outstanding_amount"]>1,1,inplace=True)
working_data["penal_interest_pending"].mask(working_data["penal_interest_pending"]>1,1,inplace=True)
working_data["penalty_charges_pending"].mask(working_data["penalty_charges_pending"]>1,1,inplace=True)
working_data["Total_Paid"].mask(working_data["Total_Paid"]>1,1,inplace=True)
working_data["disp_code_paid"].mask(working_data["disp_code_paid"]>1,1,inplace=True)
working_data["disp_code_settlement"].mask(working_data["disp_code_settlement"]>1,1,inplace=True)
working_data["TimeWeightedTotalPercentPaid"].mask(working_data["TimeWeightedTotalPercentPaid"]>1,1,inplace=True)
working_data["total_outstanding"].mask(working_data["total_outstanding"]>1,1,inplace=True)
working_data["total_outstanding"].mask(working_data["total_outstanding"]<0,0,inplace=True)
working_data["Validated income"].mask(working_data["Validated income"]>1,1,inplace=True)
working_data["Sanction amount"].mask(working_data["Sanction amount"]>1,1,inplace=True)
working_data["Amount disbursed"].mask(working_data["Amount disbursed"]>1,1,inplace=True)
working_data["Count of previous loans"].mask(working_data["Count of previous loans"]>1,1,inplace=True)

# for x in working_data.columns:
#   if working_data[(working_data.x>1)]:
#     working_data[x].mask(working_data[x]>1, 1, inplace = True)
#   elif working_data[x]<0:
#     working_data[x].replace(working_data[x]<0, 0)
#   elif 1>working_data[x]>0:
#     pass

normalised_df=working_data.copy()

normalised_df

normalised_df.to_csv("Navi_Normalised_df.csv", index = False)

normalised_df.to_sql("normalised_data",con=engine, if_exists='replace',index=False)

"""End of Normalisation"""